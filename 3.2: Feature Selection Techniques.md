# **📌 Lecture 3.2: Feature Selection Techniques**  

## **📢 Learning Objectives:**  
By the end of this lecture, you will learn:  
✅ The importance of feature selection in machine learning  
✅ Different feature selection techniques  
✅ How to apply methods like **Variance Threshold, Correlation-based Selection, Chi-Square Test, and Principal Component Analysis (PCA)**  

---

## **🔹 1. Introduction to Feature Selection**  

📌 **What is Feature Selection?**  
Feature selection is the process of **choosing the most relevant features (columns)** from a dataset to improve model performance.  

💡 **Why is Feature Selection Important?**  
✔️ **Reduces overfitting** (removes irrelevant/noisy features)  
✔️ **Improves accuracy** (focuses on important variables)  
✔️ **Speeds up model training** (reduces computational cost)  

---

## **🔹 2. Types of Feature Selection Methods**  

Feature selection methods can be categorized into:  
1️⃣ **Filter Methods** – Select features based on statistical techniques  
2️⃣ **Wrapper Methods** – Use model performance to choose features  
3️⃣ **Embedded Methods** – Feature selection occurs during model training  

---

## **🔹 3. Filter Methods**  
📌 **Filter methods use statistical techniques** to evaluate feature importance **before training a model**.  

### **🔹 3.1: Removing Low Variance Features (Variance Threshold)**  
✔️ Features with **low variance** (same values for most rows) contribute **little to model performance**.  
✔️ The `VarianceThreshold()` method from `sklearn.feature_selection` removes them.  

#### ✅ **Example: Removing Low Variance Features**
```python
import pandas as pd
from sklearn.feature_selection import VarianceThreshold

# Sample dataset
data = {'Feature_1': [0, 0, 0, 0, 1, 0, 0, 0],  # Low variance
        'Feature_2': [10, 15, 10, 12, 14, 10, 10, 11],  # Higher variance
        'Feature_3': [5, 5, 5, 5, 5, 5, 5, 5]}  # Constant (zero variance)

df = pd.DataFrame(data)

# Apply variance threshold
selector = VarianceThreshold(threshold=0.1)
df_reduced = selector.fit_transform(df)

print(df_reduced)
```
✔️ **Removes columns with variance below 0.1**  

---

### **🔹 3.2: Correlation-Based Feature Selection**  
📌 **Highly correlated features** contain redundant information.  
✔️ If correlation > 0.9, remove one of the correlated features.  

#### ✅ **Example: Identifying Correlated Features**
```python
import seaborn as sns
import matplotlib.pyplot as plt

# Compute correlation matrix
corr_matrix = df.corr()

# Plot heatmap
plt.figure(figsize=(5, 3))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()
```
✔️ Helps in **detecting redundant features**.  

---

### **🔹 3.3: Chi-Square Test for Categorical Features**  
📌 The **Chi-Square Test** measures dependency between categorical features and the target variable.  

#### ✅ **Example: Selecting Features using Chi-Square Test**
```python
from sklearn.feature_selection import chi2, SelectKBest
import numpy as np

# Sample categorical dataset
X = np.array([[0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 1]])
y = np.array([1, 1, 0, 0])  # Target variable

# Apply Chi-Square test
selector = SelectKBest(score_func=chi2, k=2)  # Select top 2 features
X_new = selector.fit_transform(X, y)

print(X_new)
```
✔️ Selects the **most important categorical features**.  

---

## **🔹 4. Wrapper Methods**  
📌 Wrapper methods **train a model multiple times** to find the best features.  
✔️ They are computationally expensive but **highly accurate**.  

### **🔹 4.1: Recursive Feature Elimination (RFE)**  
✔️ RFE removes **least important features** one by one.  

#### ✅ **Example: Using RFE with Logistic Regression**
```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Sample dataset
X = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
y = [0, 1, 0, 1]  # Binary target variable

# Define model
model = LogisticRegression()

# Apply RFE
selector = RFE(model, n_features_to_select=2)
X_reduced = selector.fit_transform(X, y)

print(X_reduced)
```
✔️ **Removes less important features automatically**.  

---

## **🔹 5. Embedded Methods**  
📌 These methods **select features while training the model**.  

### **🔹 5.1: Feature Importance with Decision Trees**  
✔️ **Tree-based models** (Random Forest, XGBoost) assign **importance scores** to features.  

#### ✅ **Example: Using Feature Importance in Random Forest**
```python
from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest model
model = RandomForestClassifier()
model.fit(X, y)

# Get feature importance scores
importance = model.feature_importances_

print(importance)
```
✔️ **Higher scores indicate more important features**.  

---

## **🔹 6. Principal Component Analysis (PCA)**  
📌 **PCA reduces dimensionality** while preserving **important information**.  

✔️ Converts original features into **principal components**  
✔️ Helps with **high-dimensional data**  

#### ✅ **Example: Applying PCA**
```python
from sklearn.decomposition import PCA

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(X_pca)
```
✔️ **Reduces feature count while retaining variance**.  

---

## **🔹 7. Summary**  
✅ **Filter Methods**: Variance Threshold, Correlation Analysis, Chi-Square Test  
✅ **Wrapper Methods**: Recursive Feature Elimination (RFE)  
✅ **Embedded Methods**: Decision Tree Feature Importance  
✅ **Dimensionality Reduction**: PCA  

---

## **🔹 8. Assignment**  
📌 **Task 1:** Apply Variance Threshold on a dataset  
📌 **Task 2:** Use correlation analysis to remove highly correlated features  
📌 **Task 3:** Apply PCA and visualize principal components  

---
