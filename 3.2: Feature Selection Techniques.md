# **ğŸ“Œ Lecture 3.2: Feature Selection Techniques**  

## **ğŸ“¢ Learning Objectives:**  
By the end of this lecture, you will learn:  
âœ… The importance of feature selection in machine learning  
âœ… Different feature selection techniques  
âœ… How to apply methods like **Variance Threshold, Correlation-based Selection, Chi-Square Test, and Principal Component Analysis (PCA)**  

---

## **ğŸ”¹ 1. Introduction to Feature Selection**  

ğŸ“Œ **What is Feature Selection?**  
Feature selection is the process of **choosing the most relevant features (columns)** from a dataset to improve model performance.  

ğŸ’¡ **Why is Feature Selection Important?**  
âœ”ï¸ **Reduces overfitting** (removes irrelevant/noisy features)  
âœ”ï¸ **Improves accuracy** (focuses on important variables)  
âœ”ï¸ **Speeds up model training** (reduces computational cost)  

---

## **ğŸ”¹ 2. Types of Feature Selection Methods**  

Feature selection methods can be categorized into:  
1ï¸âƒ£ **Filter Methods** â€“ Select features based on statistical techniques  
2ï¸âƒ£ **Wrapper Methods** â€“ Use model performance to choose features  
3ï¸âƒ£ **Embedded Methods** â€“ Feature selection occurs during model training  

---

## **ğŸ”¹ 3. Filter Methods**  
ğŸ“Œ **Filter methods use statistical techniques** to evaluate feature importance **before training a model**.  

### **ğŸ”¹ 3.1: Removing Low Variance Features (Variance Threshold)**  
âœ”ï¸ Features with **low variance** (same values for most rows) contribute **little to model performance**.  
âœ”ï¸ The `VarianceThreshold()` method from `sklearn.feature_selection` removes them.  

#### âœ… **Example: Removing Low Variance Features**
```python
import pandas as pd
from sklearn.feature_selection import VarianceThreshold

# Sample dataset
data = {'Feature_1': [0, 0, 0, 0, 1, 0, 0, 0],  # Low variance
        'Feature_2': [10, 15, 10, 12, 14, 10, 10, 11],  # Higher variance
        'Feature_3': [5, 5, 5, 5, 5, 5, 5, 5]}  # Constant (zero variance)

df = pd.DataFrame(data)

# Apply variance threshold
selector = VarianceThreshold(threshold=0.1)
df_reduced = selector.fit_transform(df)

print(df_reduced)
```
âœ”ï¸ **Removes columns with variance below 0.1**  

---

### **ğŸ”¹ 3.2: Correlation-Based Feature Selection**  
ğŸ“Œ **Highly correlated features** contain redundant information.  
âœ”ï¸ If correlation > 0.9, remove one of the correlated features.  

#### âœ… **Example: Identifying Correlated Features**
```python
import seaborn as sns
import matplotlib.pyplot as plt

# Compute correlation matrix
corr_matrix = df.corr()

# Plot heatmap
plt.figure(figsize=(5, 3))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()
```
âœ”ï¸ Helps in **detecting redundant features**.  

---

### **ğŸ”¹ 3.3: Chi-Square Test for Categorical Features**  
ğŸ“Œ The **Chi-Square Test** measures dependency between categorical features and the target variable.  

#### âœ… **Example: Selecting Features using Chi-Square Test**
```python
from sklearn.feature_selection import chi2, SelectKBest
import numpy as np

# Sample categorical dataset
X = np.array([[0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 0, 1]])
y = np.array([1, 1, 0, 0])  # Target variable

# Apply Chi-Square test
selector = SelectKBest(score_func=chi2, k=2)  # Select top 2 features
X_new = selector.fit_transform(X, y)

print(X_new)
```
âœ”ï¸ Selects the **most important categorical features**.  

---

## **ğŸ”¹ 4. Wrapper Methods**  
ğŸ“Œ Wrapper methods **train a model multiple times** to find the best features.  
âœ”ï¸ They are computationally expensive but **highly accurate**.  

### **ğŸ”¹ 4.1: Recursive Feature Elimination (RFE)**  
âœ”ï¸ RFE removes **least important features** one by one.  

#### âœ… **Example: Using RFE with Logistic Regression**
```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Sample dataset
X = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
y = [0, 1, 0, 1]  # Binary target variable

# Define model
model = LogisticRegression()

# Apply RFE
selector = RFE(model, n_features_to_select=2)
X_reduced = selector.fit_transform(X, y)

print(X_reduced)
```
âœ”ï¸ **Removes less important features automatically**.  

---

## **ğŸ”¹ 5. Embedded Methods**  
ğŸ“Œ These methods **select features while training the model**.  

### **ğŸ”¹ 5.1: Feature Importance with Decision Trees**  
âœ”ï¸ **Tree-based models** (Random Forest, XGBoost) assign **importance scores** to features.  

#### âœ… **Example: Using Feature Importance in Random Forest**
```python
from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest model
model = RandomForestClassifier()
model.fit(X, y)

# Get feature importance scores
importance = model.feature_importances_

print(importance)
```
âœ”ï¸ **Higher scores indicate more important features**.  

---

## **ğŸ”¹ 6. Principal Component Analysis (PCA)**  
ğŸ“Œ **PCA reduces dimensionality** while preserving **important information**.  

âœ”ï¸ Converts original features into **principal components**  
âœ”ï¸ Helps with **high-dimensional data**  

#### âœ… **Example: Applying PCA**
```python
from sklearn.decomposition import PCA

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(X_pca)
```
âœ”ï¸ **Reduces feature count while retaining variance**.  

---

## **ğŸ”¹ 7. Summary**  
âœ… **Filter Methods**: Variance Threshold, Correlation Analysis, Chi-Square Test  
âœ… **Wrapper Methods**: Recursive Feature Elimination (RFE)  
âœ… **Embedded Methods**: Decision Tree Feature Importance  
âœ… **Dimensionality Reduction**: PCA  

---

## **ğŸ”¹ 8. Assignment**  
ğŸ“Œ **Task 1:** Apply Variance Threshold on a dataset  
ğŸ“Œ **Task 2:** Use correlation analysis to remove highly correlated features  
ğŸ“Œ **Task 3:** Apply PCA and visualize principal components  

---
