### **Lecture 3: Unsupervised Learning**

#### **1. Lecture Objectives**

* Define unsupervised learning and its characteristics
* Explore key techniques: Clustering and Dimensionality Reduction
* Understand commonly used algorithms
* Visualize unsupervised learning with real-world examples
* Demonstrate clustering with Python (optional practical segment)

---

### **2. What is Unsupervised Learning?**

> **Definition:** Unsupervised learning is a type of machine learning where the model learns patterns and structures from data **without labeled outputs**.

#### **Key Characteristics**

* No labels; only input features
* Finds hidden patterns or groupings in data
* Used for exploratory data analysis

---

### **3. Applications of Unsupervised Learning**

* Customer segmentation
* Market basket analysis
* Document classification
* Anomaly detection
* Recommender systems
* Data compression

---

### **4. Techniques in Unsupervised Learning**

#### **A. Clustering**

* **Goal:** Group data into clusters based on similarity
* **Common Algorithms:**

  * **K-Means Clustering**
  * **Hierarchical Clustering**
  * **DBSCAN (Density-Based Spatial Clustering)**

#### **B. Dimensionality Reduction**

* **Goal:** Reduce the number of features while preserving important information
* **Common Techniques:**

  * **Principal Component Analysis (PCA)**
  * **t-Distributed Stochastic Neighbor Embedding (t-SNE)**

---

### **5. K-Means Clustering: A Closer Look**

#### **Steps:**

1. Choose number of clusters $K$
2. Randomly initialize $K$ centroids
3. Assign points to nearest centroid
4. Recalculate centroids
5. Repeat until convergence

#### **Key Concepts:**

* **Inertia (Within-Cluster Sum of Squares):** Used to evaluate clustering quality
* **Elbow Method:** Helps determine optimal number of clusters

---

### **6. Dimensionality Reduction: PCA**

* PCA projects high-dimensional data to lower dimensions
* Finds **principal components**: directions of maximum variance
* Helps with:

  * Visualization
  * Noise reduction
  * Faster model training

---

### **7. Python Demo: K-Means Clustering**

```python
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load dataset
data = load_iris()
X = data.data

# Apply KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# Plot (using first two features)
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("K-Means Clustering")
plt.show()
```

---

### **8. Evaluation of Unsupervised Models**

* **Clustering Metrics:**

  * Silhouette Score
  * Daviesâ€“Bouldin Index
  * Dunn Index
* **Visualization:** Always inspect cluster separation visually if possible

---

### **9. Summary**

* Unsupervised learning reveals hidden patterns without labeled data
* Clustering groups similar items together
* Dimensionality reduction simplifies data representation
* These techniques are essential in exploratory analysis and data preprocessing

---

### **10. Assignment**

* Apply K-Means clustering to a dataset of your choice
* Visualize the clusters and use the elbow method to justify the number of clusters

